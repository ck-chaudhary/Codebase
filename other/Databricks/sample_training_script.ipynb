{"cells":[{"cell_type":"markdown","source":["# Binary Classification Example"],"metadata":{}},{"cell_type":"markdown","source":["The Pipelines API provides higher-level API built on top of DataFrames for constructing ML pipelines.\nYou can read more about the Pipelines API in the [programming guide](https://spark.apache.org/docs/latest/ml-guide.html).\n\n**Binary Classification** is the task of predicting a binary label.\nE.g., is an email spam or not spam? Should I show this ad to this user or not? Will it rain tomorrow or not?\nThis section demonstrates algorithms for making these types of predictions."],"metadata":{}},{"cell_type":"markdown","source":["## Dataset Review"],"metadata":{}},{"cell_type":"markdown","source":["The Adult dataset we are going to use is publicly available at the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Adult).\nThis data derives from census data, and consists of information about 48842 individuals and their annual income.\nWe will use this information to predict if an individual earns >50k a year or <=50K a year.\nThe dataset is rather clean, and consists of both numeric and categorical variables.\n\nAttribute Information:\n\n- age: continuous\n- workclass: Private,Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n- fnlwgt: continuous\n- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc...\n- education-num: continuous\n- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent...\n- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners...\n- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n- sex: Female, Male\n- capital-gain: continuous\n- capital-loss: continuous\n- hours-per-week: continuous\n- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany...\n\nTarget/Label: - <=50K, >50K"],"metadata":{}},{"cell_type":"markdown","source":["## Load Data"],"metadata":{}},{"cell_type":"markdown","source":["In this example, we will read in the Adult dataset from databricks-datasets.\nWe'll read in the data in SQL using the CSV data source for Spark and rename the columns appropriately."],"metadata":{}},{"cell_type":"code","source":["%fs ls databricks-datasets/adult/adult.data"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql DROP TABLE IF EXISTS adult"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sql\nCREATE TABLE adult (\n  age DOUBLE,\n  workclass STRING,\n  fnlwgt DOUBLE,\n  education STRING,\n  education_num DOUBLE,\n  marital_status STRING,\n  occupation STRING,\n  relationship STRING,\n  race STRING,\n  sex STRING,\n  capital_gain DOUBLE,\n  capital_loss DOUBLE,\n  hours_per_week DOUBLE,\n  native_country STRING,\n  income STRING)\nUSING com.databricks.spark.csv\nOPTIONS (path \"/databricks-datasets/adult/adult.data\", header \"true\")\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["dataset = spark.table(\"adult\")\ncols = dataset.columns"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display(dataset)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Preprocess Data\n\nSince we are going to try algorithms like Logistic Regression, we will have to convert the categorical variables in the dataset into numeric variables.\nThere are 2 ways we can do this.\n\n* Category Indexing\n\n  This is basically assigning a numeric value to each category from {0, 1, 2, ...numCategories-1}.\n  This introduces an implicit ordering among your categories, and is more suitable for ordinal variables (eg: Poor: 0, Average: 1, Good: 2)\n\n* One-Hot Encoding\n\n  This converts categories into binary vectors with at most one nonzero value (eg: (Blue: [1, 0]), (Green: [0, 1]), (Red: [0, 0]))\n\nIn this dataset, we have ordinal variables like education (Preschool - Doctorate), and also nominal variables like relationship (Wife, Husband, Own-child, etc).\nFor simplicity's sake, we will use One-Hot Encoding to convert all categorical variables into binary vectors.\nIt is possible here to improve prediction accuracy by converting each categorical column with an appropriate method.\n\nHere, we will use a combination of [StringIndexer] and [OneHotEncoderEstimator] to convert the categorical variables.\nThe `OneHotEncoderEstimator` will return a [SparseVector].\n\nSince we will have more than 1 stages of feature transformations, we use a [Pipeline] to tie the stages together.\nThis simplifies our code.\n\n[StringIndexer]: http://spark.apache.org/docs/latest/ml-features.html#stringindexer\n[OneHotEncoderEstimator]: https://spark.apache.org/docs/latest/ml-features.html#onehotencoderestimator\n[SparseVector]: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.linalg.SparseVector\n[Pipeline]: http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\ncategoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["The above code basically indexes each categorical column using the `StringIndexer`,\nand then converts the indexed categories into one-hot encoded variables.\nThe resulting output has the binary vectors appended to the end of each row.\n\nWe use the `StringIndexer` again to encode our labels to label indices."],"metadata":{}},{"cell_type":"code","source":["# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"income\", outputCol=\"label\")\nstages += [label_stringIdx]"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Next, we will use the `VectorAssembler` to combine all the feature columns into a single vector column.\nThis will include both the numeric columns and the one-hot encoded binary vector columns in our dataset."],"metadata":{}},{"cell_type":"code","source":["# Transform all features into a vector using VectorAssembler\nnumericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["We finally run our stages as a Pipeline.\nThis puts the data through all of the feature transformations we described in a single call."],"metadata":{}},{"cell_type":"code","source":["# Create a Pipeline.\npipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(dataset)\ndataset = pipelineModel.transform(dataset)\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = dataset.select(selectedcols)\ndisplay(dataset)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\nprint(trainingData.count())\nprint(testData.count())"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Fit and Evaluate Models\n\nWe are now ready to try out some of the Binary Classification algorithms available in the Pipelines API.\n\nOut of these algorithms, the below are also capable of supporting multiclass classification with the Python API:\n- Decision Tree Classifier\n- Random Forest Classifier\n\nThese are the general steps we will take to build our models:\n- Create initial model using the training set\n- Tune parameters with a `ParamGrid` and 5-fold Cross Validation\n- Evaluate the best model obtained from the Cross Validation using the test set\n\nWe use the `BinaryClassificationEvaluator` to evaluate our models, which uses [areaUnderROC] as the default metric.\n\n[areaUnderROC]: https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve"],"metadata":{}},{"cell_type":"markdown","source":["## Logistic Regression\n\nYou can read more about [Logistic Regression] from the [classification and regression] section of MLlib Programming Guide.\nIn the Pipelines API, we are now able to perform Elastic-Net Regularization with Logistic Regression, as well as other linear methods.\n\n[classification and regression]: https://spark.apache.org/docs/latest/ml-classification-regression.html\n[Logistic Regression]: https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["sparkTransformed = lrModel.transform(trainingData)\ndisplay(sparkTransformed)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Make predictions on test data using the transform() method.\n# LogisticRegression.transform() will only use the 'features' column.\npredictions = lrModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["predictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["We can use ``BinaryClassificationEvaluator`` to evaluate our model. We can set the required column names in `rawPredictionCol` and `labelCol` Param and the metric in `metricName` Param."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Note that the default metric for the ``BinaryClassificationEvaluator`` is ``areaUnderROC``"],"metadata":{}},{"cell_type":"code","source":["evaluator.getMetricName()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# MLeap supports serializing the model to one zip file. In order to serialize to a zip file, make sure the URI begins with jar:file and ends with a .zip.\n# The supported transformers for MLeap can be found in the MLeap documentation."],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["%sh \nrm -rf /tmp/mleap_python_model_export\nmkdir /tmp/mleap_python_model_export"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["import mleap.pyspark\nfrom mleap.pyspark.spark_support import SimpleSparkSerializer\n\nlrModel.serializeToBundle(\"jar:file:/tmp/mleap_python_model_export/binaryclassifier_pipeline-json.zip\", sparkTransformed)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["#In this example we download the model files from the browser. In general, you may want to programmatically move the model to a persistent storage layer."],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%sh\ncp /tmp/mleap_python_model_export/binaryclassifier_pipeline-json.zip /dbfs/FileStore/binaryclassifier_pipeline-json.zip"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["#Get a link to the downloadable zip via: https://[MY_DATABRICKS_URL]/files/[FILE_NAME].zip. E.g., if you access Databricks at https://mycompany.databricks.com, then your link would be: https://mycompany.databricks.com/files/binaryclassifier_pipeline-json.zip."],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["#This section shows how to use MLeap to load a trained model for use in your application. To use an existing ML models and pipelines to make predictions for new data, you can deserialize the model from the file you saved.\n\n#Import Model to PySpark\n#This section shows how to load an MLeap bundle and make predictions on a Spark DataFrame. This can be useful if you want to use the same persistence format (bundle) for loading into Spark and non-Spark applications. If your goal is to make predictions only in Spark, then we recommend using MLlib's native ML persistence."],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["from pyspark.ml import PipelineModel\ndeserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:/tmp/mleap_python_model_export/binaryclassifier_pipeline-json.zip\")"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["#Now you can use the loaded model to make predictions.\ndisplay(testData)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["exampleResults = deserializedPipeline.transform(testData)\ndisplay(exampleResults)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["#check if the file is /tmp/ folder\ndisplay(dbutils.fs.ls(\"file:/tmp/mleap_python_model_export/\"))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["dbutils.fs.mkdirs(\"/FileStore/model\")\ndbutils.fs.cp(\"file:/tmp/mleap_python_model_export/binaryclassifier_pipeline-json.zip\", \"/FileStore/model/binaryclassifier_pipeline-json.zip\")"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/FileStore/model\"))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":45}],"metadata":{"name":"sample_training_script","notebookId":3764659880469121},"nbformat":4,"nbformat_minor":0}
